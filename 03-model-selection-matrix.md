# RK3576 模型选择矩阵 (基于研究)

*注意: 由于环境问题跳过了本地基准测试。推荐基于 RK3576 规格 (6 TOPS, 8GB RAM) 和社区数据。*

## 1. 首选推荐: 通用聊天 / 助手
- **模型**: **Qwen2.5-1.5B-Instruct** (W4A16 或 W8A8 量化)
- **理由**: 在小参数规模下，速度、质量和指令遵循的最佳平衡。
- **预估速度**: ~15-20 tok/s。
- **内存**: ~1.5GB (为系统/应用留有充足空间)。

## 2. 最佳推理 / 代码
- **模型**: **DeepSeek-R1-Distill-Qwen-1.5B**
- **理由**: 小体积中蕴含蒸馏后的推理能力。
- **预估速度**: ~12-18 tok/s。
- **注意**: 较新的模型，确保使用 RKLLM v1.2.3+。

## 3. 最佳英文 / 创意写作
- **模型**: **Phi-3-Mini-3.8B** (W4A16 量化)
- **理由**: 基于合成数据训练，高质量文本生成。
- **预估速度**: ~5-8 tok/s (由于体积较大，速度较慢)。
- **内存**: ~3.5GB。

## 4. 最佳超低延迟 / IoT 控制
- **模型**: **TinyLlama-1.1B** 或 **Qwen2.5-0.5B**
- **理由**: 极快，低内存占用。适合简单的 JSON 提取或分类。
- **预估速度**: 30+ tok/s。
- **内存**: < 1GB。

## 5. 视觉语言模型 (VLM) 选项
- **模型**: **Qwen2-VL-2B** 或 **MiniCPM-V-2.6**
- **理由**: 此类硬件级别唯一可行的 VLM。
- **状态**: RKLLM 支持。通常需要 C++ demo 以获得最佳性能。

## 6. 部署策略
| 用例 | 推荐模型 | 量化 | 备注 |
| :--- | :--- | :--- | :--- |
| **个人助手** | Qwen2.5-1.5B | W4A16 / W8A8 | 快速，良好的聊天体验。 |
| **编程助手** | DeepSeek-R1-Distill | W4A16 | 逻辑性好，适合内存。 |
| **家庭自动化** | Qwen2.5-0.5B | W8A8 | 即时响应。 |
| **阅读长文本** | Phi-3-Mini | W4A16 | 更好的连贯性 (3.8B)。 |
